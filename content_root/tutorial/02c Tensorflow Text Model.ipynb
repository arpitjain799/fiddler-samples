{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Tutorial for Text Data with Integrated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Fiddler Client\n",
    "We begin this section as usual by establishing a connection to our\n",
    "Fiddler instance. We can establish this connection either by specifying \n",
    "our credentials directly, or by utilizing our `fiddler.ini` file. More\n",
    "information can be found in the [setup](https://github.com/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/00%20Install%20%26%20Setup.ipynb) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "\n",
    "# client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=auth_token)\n",
    "client = fdl.FiddlerApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'tf_text'\n",
    "dataset_id = 'imdb_rnn'\n",
    "model_id = 'imdb_rnn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a project, a convenient container for housing the models and datasets associated with a given ML use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our project using project_id\n",
    "if project_id not in client.list_projects():\n",
    "    client.create_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Here we will load in our baseline dataset from a csv called `imdb_rnn.csv`. We will\n",
    "also create a schema using this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/app/fiddler_samples/samples/datasets/imdb_rnn/imdb_rnn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub(' ', text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)    # Removing multiple spaces\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_sentences = []\n",
    "sentences = list(df['sentence'])\n",
    "for sen in sentences:\n",
    "    processed_sentences.append(preprocess_text(sen))\n",
    "\n",
    "df['sentence'] = processed_sentences\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Dataset\n",
    "To upload a model, you first need to upload a sample of the data of the model’s \n",
    "inputs, targets, and additional metadata that might be useful for model analysis. \n",
    "This data sample helps us (among other things) to infer the model schema and the \n",
    "data types and values range of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_id not in client.list_datasets(project_id):\n",
    "    upload_result = client.upload_dataset(\n",
    "        project_id=project_id,\n",
    "        dataset={'train': df}, \n",
    "        dataset_id=dataset_id,\n",
    "        info=df_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Schema\n",
    "As you must have noted, in the dataset upload step we did not ask for the model’s \n",
    "features and targets, or any model specific information. That’s because we \n",
    "allow for linking multiple models to a given dataset schema. Hence we require \n",
    "an Infer model schema step which helps us know the features relevant to the \n",
    "model and the model task. Here you can specify the input features, the target \n",
    "column, decision columns and metadata columns, and also the type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'polarity'\n",
    "feature_columns = ['sentence']\n",
    "output = 'sentiment'\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=client.get_dataset_info(project_id, dataset_id),\n",
    "    target=target,\n",
    "    features=feature_columns,\n",
    "    input_type=fdl.ModelInputType.TEXT,\n",
    "    model_task=fdl.ModelTask.BINARY_CLASSIFICATION,\n",
    "    outputs=output,\n",
    "    display_name='Text IG',\n",
    "    description='this is a tensorflow model using text data and IG enabled from tutorial',\n",
    "    preferred_explanation_method=fdl.ExplanationMethod.IG_FLEX\n",
    ")\n",
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Tensorflow if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we support Sklearn version 0.21.2 and TF version 2.5 \n",
    "If you have another version, please contact Fiddler for assistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "assert tf.__version__=='2.5.0', 'Please change tensorflow version to 2.5.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "Build and train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "train_input = df['sentence']\n",
    "\n",
    "target = 'polarity'\n",
    "train_target = df[target]\n",
    "train_target = le.fit_transform(train_target).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "vocab_size = 1000\n",
    "max_seq_length = 150\n",
    "tok = Tokenizer(num_words=vocab_size)\n",
    "tok.fit_on_texts(train_input)\n",
    "sequences = tok.texts_to_sequences(train_input)\n",
    "sequences_matrix = sequence.pad_sequences(sequences, maxlen=max_seq_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs', shape=[max_seq_length])\n",
    "    layer = Embedding(vocab_size, 64, input_length=max_seq_length)(inputs)\n",
    "    \n",
    "    \n",
    "    layer = LSTM(64, return_sequences=True)(layer)\n",
    "    layer = LSTM(32, return_sequences=True)(layer)\n",
    "    layer = GlobalAveragePooling1D()(layer)\n",
    "    layer = Dense(256, name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1, name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(sequences_matrix, train_target, batch_size=128, epochs=5,\n",
    "          validation_split=0.1, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "Next step, we need to save the model and any pre-processing step you had \n",
    "on the input features (for example Categorical encoder, Tokenization, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create model files \n",
    "model_dir = pathlib.Path(model_id)\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# save model\n",
    "model.save(model_dir/'saved_model')\n",
    "\n",
    "# save tokenizer\n",
    "with open(model_dir / 'tokenizer.pkl', 'wb') as tok_file:\n",
    "    tok_file.write(pickle.dumps(tok))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write `package.py` and related wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import related wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import the GEM wrapper for displaying the attributions. This file is stored in the utils directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('utils/GEM.py', model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write `package.py` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wrapper is needed between Fiddler and the model. This wrapper can be used to \n",
    "translate the inputs and outputs to fit what the model expects and what Fiddler \n",
    "is able to consume. This file contains functions to transform the input, generate the \n",
    "baseline and get the attributions. More information can be found [here](https://api.fiddler.ai/#package-py/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile imdb_rnn/package.py\n",
    "\n",
    "import pickle\n",
    "import pathlib\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from .GEM import GEMContainer, GEMSimple, GEMText\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Name the output of your model here - this will need to match the model schema we define in the next notebook\n",
    "OUTPUT_COL = ['sentiment']\n",
    "\n",
    "# These are the names of the inputs of yout TensorFlow model\n",
    "FEATURE_LABEL = 'sentence'\n",
    "\n",
    "MODEL_ARTIFACT_PATH = 'saved_model'\n",
    "\n",
    "TOKENIZER_PATH = 'tokenizer.pkl'\n",
    "\n",
    "ATTRIBUTABLE_LAYER_NAMES = EMBEDDING_NAMES = ['embedding']\n",
    "\n",
    "MAX_SEQ_LENGTH = 150\n",
    "\n",
    "def _pad(seq):\n",
    "    return pad_sequences(seq, MAX_SEQ_LENGTH,\n",
    "                         padding='post', truncating='post')\n",
    "\n",
    "class FiddlerModel:\n",
    "    def __init__(self):\n",
    "        \"\"\" Model deserialization and initialization goes here.  Any additional serialized preprocession\n",
    "            transformations would be initialized as well - e.g. tokenizers, embedding lookups, etc.\n",
    "        \"\"\"\n",
    "        self.model_dir = pathlib.Path(__file__).parent\n",
    "        self.model = tf.keras.models.load_model(str(self.model_dir / MODEL_ARTIFACT_PATH))\n",
    "\n",
    "        # Construct sub-models (for each ATTRIBUTABLE_LAYER_NAME)\n",
    "        # if not possible to attribute directly to the input (e.g. embeddings).\n",
    "        self.att_sub_models = {att_layer : Model(self.model.inputs,\n",
    "                    outputs=self.model.get_layer(att_layer).output)\n",
    "                    for att_layer in ATTRIBUTABLE_LAYER_NAMES}\n",
    "        \n",
    "        with open(str(self.model_dir / TOKENIZER_PATH), 'rb') as f:\n",
    "            self.tokenizer = pickle.load(f)\n",
    "        \n",
    "        self.grad_model = self._define_model_grads()\n",
    "        \n",
    "    def get_settings(self):\n",
    "        \n",
    "        return {'ig_start_steps': 32,  # 32\n",
    "                'ig_max_steps': 4096,  # 2048\n",
    "                'ig_min_error_pct':5.0 # 1.0\n",
    "               }\n",
    "        \n",
    "    def transform_to_attributable_input(self, input_df):\n",
    "        \"\"\" This method is called by the platform and is responsible for transforming the input dataframe\n",
    "            to the upstream-most representation of model inputs that belongs to a continuous vector-space.\n",
    "            For this example, the model inputs themselves meet this requirement.  For models with embedding\n",
    "            layers (esp. NLP models) the first attributable layer is downstream of that.\n",
    "        \"\"\"\n",
    "        transformed_input = self._transform_input(input_df)\n",
    "\n",
    "        return {att_layer : att_sub_model.predict(transformed_input)\n",
    "                    for att_layer, att_sub_model in self.att_sub_models.items()}\n",
    "\n",
    "    def get_ig_baseline(self, input_df):\n",
    "        \"\"\" This method is used to generate the baseline against which to compare the input. \n",
    "            It accepts a pandas DataFrame object containing rows of raw feature vectors that \n",
    "            need to be explained (in case e.g. the baseline must be sized according to the explain point).\n",
    "            Must return a pandas DataFrame that can be consumed by the predict method described earlier.\n",
    "        \"\"\"\n",
    "        baseline_df = input_df.copy()\n",
    "        baseline_df[FEATURE_LABEL] = input_df[FEATURE_LABEL].apply(lambda x: '')\n",
    "\n",
    "        return baseline_df\n",
    "\n",
    "    def _transform_input(self, input_df):\n",
    "        \"\"\" Helper function that accepts a pandas DataFrame object containing rows of raw feature vectors. \n",
    "            The output of this method can be any Python object. This function can also \n",
    "            be used to deserialize complex data types stored in dataset columns (e.g. arrays, or images \n",
    "            stored in a field in UTF-8 format).\n",
    "        \"\"\"\n",
    "        sequences = self.tokenizer.texts_to_sequences(input_df[FEATURE_LABEL])\n",
    "        sequences_matrix = sequence.pad_sequences(sequences,\n",
    "                                                  maxlen=MAX_SEQ_LENGTH,\n",
    "                                                  padding='post')\n",
    "        return sequences_matrix.tolist()\n",
    "    \n",
    "    \n",
    "    def predict(self, input_df):\n",
    "        \"\"\" Basic predict wrapper.  Takes a DataFrame of input features and returns a DataFrame\n",
    "            of predictions.\n",
    "        \"\"\"\n",
    "        transformed_input = self._transform_input(input_df)\n",
    "        pred = self.model.predict(transformed_input)\n",
    "        return pd.DataFrame(pred, columns=OUTPUT_COL)\n",
    "    \n",
    "    def compute_gradients(self, attributable_input):\n",
    "        \"\"\" This method computes gradients of the model output wrt to the differentiable input. \n",
    "            If there are embeddings, the attributable_input should be the output of the embedding \n",
    "            layer. In the backend, this method receives the output of the transform_to_attributable_input() \n",
    "            method. This must return an array of dictionaries, where each entry of the array is the attribution \n",
    "            for an output. As in the example provided, in case of single output models, this is an array with \n",
    "            single entry. For the dictionary, the key is the name of the input layer and the values are the \n",
    "            attributions.\n",
    "        \"\"\"\n",
    "        gradients_by_output = []\n",
    "        attributable_input_tensor = {k: tf.identity(v) for k, v in attributable_input.items()}\n",
    "        gradients_dic_tf = self._gradients_input(attributable_input_tensor)\n",
    "        gradients_dic_numpy = dict([key, np.asarray(value)] for key, value in gradients_dic_tf.items()) \n",
    "        gradients_by_output.append(gradients_dic_numpy)\n",
    "        return gradients_by_output    \n",
    "    \n",
    "    def _gradients_input(self, x):\n",
    "        \"\"\"\n",
    "        Function to Compute gradients.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x)\n",
    "            preds = self.grad_model(x)\n",
    "\n",
    "        grads = tape.gradient(preds, x)\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "    def _define_model_grads(self):\n",
    "        \"\"\"\n",
    "        Define a differentiable model, cut from the Embedding Layers. \n",
    "        This will take as input what the transform_to_attributable_input function defined.\n",
    "        \"\"\"\n",
    "        model = tf.keras.models.load_model(str(self.model_dir / 'saved_model'))\n",
    "\n",
    "        for index, name in enumerate(EMBEDDING_NAMES):\n",
    "            model.layers.remove(model.get_layer(name))\n",
    "            model.layers[index]._batch_input_shape = (None, 150, 64)\n",
    "            model.layers[index]._dtype = 'float32'\n",
    "            model.layers[index]._name = name\n",
    "\n",
    "        new_model = tf.keras.models.model_from_json(model.to_json())\n",
    "\n",
    "        for layer in new_model.layers:\n",
    "            try:\n",
    "                layer.set_weights(self.model.get_layer(name=layer.name).get_weights())\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return new_model\n",
    "\n",
    "\n",
    "    def project_attributions(self, input_df, attributions):\n",
    "        explanations_by_output = {}\n",
    "\n",
    "        for output_field_index, att in enumerate(attributions):           \n",
    "            segments = re.split(r'([ ' + self.tokenizer.filters + '])',\n",
    "                                input_df.iloc[0][FEATURE_LABEL])\n",
    "\n",
    "            unpadded_tokens = [self.tokenizer.texts_to_sequences([x])[0] for x\n",
    "                              in input_df[FEATURE_LABEL].values]\n",
    "\n",
    "            padded_tokens = _pad(unpadded_tokens)\n",
    "\n",
    "            word_tokens = self.tokenizer.sequences_to_texts(\n",
    "                [[x] for x in padded_tokens[0]])\n",
    "\n",
    "            # Note - summing over attributions in the embedding direction\n",
    "            word_attributions = np.sum(att['embedding'][-len(word_tokens):],\n",
    "                                       axis=1)\n",
    "\n",
    "            i = 0\n",
    "            final_attributions = []\n",
    "            final_segments = []\n",
    "            for segment in segments:\n",
    "                if segment is not '':  # dump empty tokens\n",
    "                    final_segments.append(segment)\n",
    "                    seg_low = segment.lower()\n",
    "                    if len(word_tokens) > i and seg_low == word_tokens[i]:\n",
    "                        final_attributions.append(word_attributions[i])\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        final_attributions.append(0)\n",
    "\n",
    "            gem_text = GEMText(feature_name=FEATURE_LABEL,\n",
    "                               text_segments=final_segments,\n",
    "                               text_attributions=final_attributions)\n",
    "\n",
    "            gem_container = GEMContainer(contents=[gem_text])\n",
    "\n",
    "            explanations_by_output[OUTPUT_COL[output_field_index]] \\\n",
    "                = gem_container.render()\n",
    "\n",
    "        return explanations_by_output\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    return FiddlerModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Model\n",
    "Now that we have all the parts that we need, we can go ahead and upload the model to the Fiddler platform. You first need to add your model shema in Fiddler using `add_model`. Then, you can use the `add_model_artifact` to upload this entire directory in one shot. We need the following for uploading a model:\n",
    "- The `path` to the directory\n",
    "- The `project_id` to which the model belongs\n",
    "- The `model_id`, which is the name you want to give the model. You can access it in Fiddler henceforth via this ID\n",
    "- The `dataset_id` which the model is linked to\n",
    "\n",
    "In total, we will have a model file, a tokenizer file, the GEM.py wrapper and a `package.py` file within our model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_model(project_id, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_model(project_id=project_id, model_id=model_id, dataset_id=dataset_id, model_info=model_info)\n",
    "client.add_model_artifact(model_dir=model_dir, project_id=project_id, model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model\n",
    "Now, let's test out our model by interfacing with the client and \n",
    "calling [run model](https://api.fiddler.ai/#run-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_input = df.head(3)\n",
    "client.run_model(project_id, model_id, prediction_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Explanation\n",
    "Let's get an explanation on a selected data point to better understand how our\n",
    "model came to the conclusion it did. We can do so by calling the `run_explanation`\n",
    "method. In this case, we will call for an explanation using `'ig'`.\n",
    "More information on this method can be found [here](https://api.fiddler.ai/#run-explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_point = df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.run_explanation(\n",
    "    project_id=project_id,\n",
    "    model_id=model_id, \n",
    "    df=selected_point, \n",
    "    dataset_id=dataset_id,\n",
    "    explanations='ig_flex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

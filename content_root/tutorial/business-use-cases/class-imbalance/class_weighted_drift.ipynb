{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdabc6d",
   "metadata": {},
   "source": [
    "# Fiddler examples have moved! [Deprecation Notice]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9ed62",
   "metadata": {},
   "source": [
    "Dear user thank you for using fiddler product, we appreciate your time! We have moved the examples to a new github repo located at the following link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5b370",
   "metadata": {},
   "source": [
    "***\n",
    "# [New fiddler-examples repo](https://github.com/fiddler-labs/fiddler-examples)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e3c03",
   "metadata": {},
   "source": [
    "# Detecting drift in imbalanced scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffacce67",
   "metadata": {},
   "source": [
    "In this notebook we walk through a heavily imbalanced fraud prediction example. \n",
    "- We first train a model on the heavily imbalanced data with Random over-sampling\n",
    "- We then introduce an uptick in fraud in production data. \n",
    "- Finally we add the model with Fiddler backend and use weighted aggregation to tease out the change in production distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca5214",
   "metadata": {},
   "source": [
    "## Package installation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07367d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install fiddler-client>=1.2.0\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a196f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.utils\n",
    "import collections\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import auc, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "import fiddler as fdl\n",
    "print(f\"Running client version {fdl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f6ff7",
   "metadata": {},
   "source": [
    "## Preprocessing helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e2a8be",
   "metadata": {},
   "source": [
    "Following cells preprocess the raw data before training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939bcce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    std_scaler = StandardScaler()\n",
    "    robust_scaler = RobustScaler()\n",
    "\n",
    "    df['scaled_amount'] = robust_scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "    df['scaled_time'] = robust_scaler.fit_transform(df['Time'].values.reshape(-1, 1))\n",
    "    df.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "\n",
    "    scaled_amount = df['scaled_amount']\n",
    "    scaled_time = df['scaled_time']\n",
    "\n",
    "    df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "    df.insert(0, 'scaled_amount', scaled_amount)\n",
    "    df.insert(1, 'scaled_time', scaled_time)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66084e55",
   "metadata": {},
   "source": [
    "# Dataset and Label Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227bcee",
   "metadata": {},
   "source": [
    "The dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
    "\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "Source: https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = 'https://s3.us-west-1.amazonaws.com/fiddler.ai/download/creditcard_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca5992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_URL)\n",
    "df = preprocess_data(df)\n",
    "print('Class label distribbution (in percentage): ')\n",
    "df['Class'].value_counts()/len(df)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591be456",
   "metadata": {},
   "source": [
    "### Computing class-weights\n",
    "- We will compute the class-weighting factors based on the distribution of labels in baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd1acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_WEIGHT = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(df['Class']), y=df['Class']).tolist()\n",
    "print(f'Computed class-weights: {CLASS_WEIGHT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c63cbc",
   "metadata": {},
   "source": [
    "## Random Undersampling helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c22a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sample(df, target_col):\n",
    "    target_dist = df[target_col].value_counts()\n",
    "    targets = list(target_dist.index)\n",
    "    min_samples = target_dist.min()\n",
    "    sampled_dfs = []\n",
    "    for target_key in targets:\n",
    "        target_slice = df.loc[df[target_col]==target_key][:min_samples]\n",
    "        sampled_dfs.append(target_slice)\n",
    "    new_df = pd.concat(sampled_dfs)\n",
    "    return new_df.sample(frac=1, random_state=42, replace=False)\n",
    "\n",
    "def prepare_train_val(df, target_col, val_size=0.05):\n",
    "    data_split = train_test_split(X, y, test_size=val_size, random_state=RANDOM_STATE)\n",
    "    for dat_df in data_split:\n",
    "        dat_df.reset_index(drop=True, inplace=True)\n",
    "    return data_split\n",
    "\n",
    "def prepare_test(df, target_col):\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d97ee",
   "metadata": {},
   "source": [
    "## Model training helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d750e9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_obj, train_df, test_df, target_col='Class'):\n",
    "    X_train = train_df.drop(target_col, axis=1).values\n",
    "    y_train = train_df[target_col].values\n",
    "    \n",
    "    X_test = test_df.drop(target_col, axis=1).values\n",
    "    y_test = test_df[target_col].values\n",
    "    \n",
    "    print(f'Training the model with {len(X_train)} samples')\n",
    "    model_obj.fit(X_train, y_train)\n",
    "    calc_roc_auc, calc_pr_auc = get_metrics(model_obj, X_test, y_test)\n",
    "    print(\n",
    "        f\"Classsifier {model_obj.__class__.__name__} type has PR-AUC score of {calc_pr_auc} on test set\"\n",
    "    )\n",
    "    return model_obj\n",
    "\n",
    "def get_metrics(model_obj, X_test, y_test):\n",
    "    y_pred = model_obj.predict(X_test)\n",
    "    y_scores = model_obj.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores, pos_label=1)\n",
    "    # print(fpr,  tpr, thresholds)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
    "    # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "    return roc_auc_score(y_test, y_pred), auc(recall, precision)\n",
    "\n",
    "def get_predictions(model_obj, X, y):\n",
    "    y_proba = pd.DataFrame.from_dict(\n",
    "        {\n",
    "            'prediction_score': model_obj.predict_proba(X)[:,1],\n",
    "            'Class': y,\n",
    "        }\n",
    "    )\n",
    "    y_proba.reset_index(drop=True, inplace=True)\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    pred_train_df = pd.concat([X, y_proba], axis=1)\n",
    "    return pred_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da39b6",
   "metadata": {},
   "source": [
    "# Train model with Random-Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58eb0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(train_df.drop(['Class'], axis=1), train_df['Class'])\n",
    "train_df_oversampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "\n",
    "print(sorted(Counter(y_resampled).items()))\n",
    "\n",
    "model_obj = RandomForestClassifier()\n",
    "print('Traning the model. This might take a couple of minutes...')\n",
    "model_obj = train_model(model_obj, train_df_oversampled, test_df, target_col='Class')\n",
    "\n",
    "# get predictions from the trained model\n",
    "X_baseline, y_baseline = prepare_test(train_df, target_col='Class')\n",
    "baseline_pred = get_predictions(model_obj, X_baseline, y_baseline)\n",
    "\n",
    "X_test, y_test = prepare_test(test_df, target_col='Class')\n",
    "test_pred = get_predictions(model_obj, X_test, y_test)\n",
    "t1 = time.time()\n",
    "print(f'Time required to train the model: {(t1-t0)/60.} mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27f3c2",
   "metadata": {},
   "source": [
    "# Simulating increase in fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a836c",
   "metadata": {},
   "source": [
    "- To simulate increase in fraid we will oversample the minority class (Label being fraud) by a factor of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b53333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_fraud_uptick(\n",
    "    df,\n",
    "    target_col='Class',\n",
    "    prediction_col = 'prediction_score',\n",
    "    ratio=3.0,\n",
    "):\n",
    "    minority_class_df = df.loc[df[target_col]==1]\n",
    "    majority_class_df = df.loc[df[target_col]==0]\n",
    "    \n",
    "    resampled_minority_class_df = minority_class_df.sample(frac=ratio, replace=ratio>1, random_state=RANDOM_STATE)\n",
    "    resampled_df = pd.concat(\n",
    "        [\n",
    "            resampled_minority_class_df,\n",
    "            majority_class_df\n",
    "        ]\n",
    "    )\n",
    "    return resampled_df\n",
    "uptick_fraud_df = simulate_fraud_uptick(test_pred)\n",
    "uptick_fraud_df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6f1e8",
   "metadata": {},
   "source": [
    "# Upload Baseline and Add Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de5ced",
   "metadata": {},
   "source": [
    "## Connect to Fiddler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f4dfdd",
   "metadata": {},
   "source": [
    "- Please change the following parameter to the server URL in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc72229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection parameters\n",
    "org_id =  'imbalbash'\n",
    "url = 'https://imbalbash.dev.fiddler.ai/'\n",
    "auth_token= '_2ZDDooNZRhvw43MnVEMWCLA09CuTOC8KpV9COmeEz8'\n",
    "client = fdl.FiddlerApi(url, org_id, auth_token, version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb874ead",
   "metadata": {},
   "source": [
    "### Set project, dataset and model ids\n",
    "- Let's set-up project, model and dataset names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'imbalanced_drift'\n",
    "dataset_id = 'fraud_dataset'\n",
    "model_id = 'oversampled_model'\n",
    "\n",
    "BINARY_THRESHOLD = 0.4\n",
    "TARGET_COL = 'Class'\n",
    "OUTPUT_COL = 'prediction_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae503aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up_entities = False\n",
    "if clean_up_entities:\n",
    "    logger.info(f'Deleting model: {model_id} from project: {project_name}')\n",
    "    client.delete_model(project_name, model_id, delete_prod=True)\n",
    "if clean_up_entities:\n",
    "    logger.info(f'Deleting dataset: {dataset_id} from project: {project_name}')\n",
    "    client.delete_dataset(project_name, dataset_id)\n",
    "if clean_up_entities:\n",
    "    logger.info(f'Deleting project: {project_name}')\n",
    "    client.delete_project(project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da5cd1a",
   "metadata": {},
   "source": [
    "### Upload baseline dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff77b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = baseline_pred.sample(frac=0.1, random_state=RANDOM_STATE)\n",
    "event_df = uptick_fraud_df.sample(frac=0.1, random_state=RANDOM_STATE)\n",
    "print(f'Adding model with {len(baseline_df)} samples in baseline ' \n",
    "      f'and {len(event_df)} samples in production' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not project_name in client.list_projects():\n",
    "    logger.info(f'Creating project: {project_name}')\n",
    "    client.create_project(project_name)\n",
    "else:\n",
    "    logger.info(f'Project: {project_name} already exists')\n",
    "\n",
    "baseline_dataset_info = fdl.DatasetInfo.from_dataframe(\n",
    "    baseline_pred,\n",
    "    dataset_id=dataset_id,\n",
    "    max_inferred_cardinality=100\n",
    ")\n",
    "\n",
    "if not dataset_id in client.list_datasets(project_id=project_name):\n",
    "    logger.info(f'Upload dataset {dataset_id}')\n",
    "    resp = client.upload_dataset(\n",
    "        project_id=project_name, \n",
    "        dataset_id=dataset_id, \n",
    "        dataset={'baseline': baseline_df},\n",
    "        info=baseline_dataset_info,\n",
    "    )\n",
    "else:\n",
    "    logger.info(f'Dataset: {dataset_id} already exists in Project: {project_name}')\n",
    "baseline_dataset_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3843833",
   "metadata": {},
   "source": [
    "### Add Model\n",
    "- We will add 2 models, with and without class-weighting parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mid in [model_id + '_weighted', model_id]:\n",
    "    if 'weighted' in mid:\n",
    "        weighting_params = fdl.WeightingParams(class_weight=CLASS_WEIGHT)\n",
    "        logger.info(f'Adding model with weighting parameters.')\n",
    "    else:\n",
    "        weighting_params = None\n",
    "        logger.info(f'Adding model without weighting parameters.')\n",
    "    target_col = TARGET_COL\n",
    "    output_col = OUTPUT_COL\n",
    "    inp_features = set(baseline_df.columns) - set([target_col, output_col])\n",
    "    model_info = fdl.ModelInfo.from_dataset_info(\n",
    "        dataset_info=baseline_dataset_info,\n",
    "        target=target_col,\n",
    "        dataset_id= dataset_id,\n",
    "        features=inp_features,\n",
    "        display_name='Fraud model',\n",
    "        description='Fraud model with predictions in baseline',\n",
    "        input_type=fdl.core_objects.ModelInputType.TABULAR,\n",
    "        model_task=fdl.core_objects.ModelTask.BINARY_CLASSIFICATION,\n",
    "        outputs=output_col,\n",
    "        weighting_params=weighting_params,\n",
    "        binary_classification_threshold=BINARY_THRESHOLD,\n",
    "        categorical_target_class_details=[0, 1],\n",
    "\n",
    "    )\n",
    "    if not model_id in client.list_models(project_id=project_name):\n",
    "        client.add_model(project_id=project_name, model_id=mid, dataset_id=dataset_id, model_info=model_info)\n",
    "    else:\n",
    "        logger.info(f'Model: {mid} already exists in Project: {project_name}')\n",
    "\n",
    "    t0 = time.time()\n",
    "    logger.info('Publishing events...')\n",
    "    client.publish_events_batch(\n",
    "        project_id=project_name,\n",
    "        model_id=mid,\n",
    "        batch_source=event_df,\n",
    "    )\n",
    "    t1 = time.time()\n",
    "    dt = t1-t0\n",
    "    print(f'Time required: {dt} secs for {len(uptick_fraud_df)} events. [{len(uptick_fraud_df)/dt} events/sec]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c14171",
   "metadata": {},
   "outputs": [],
   "source": [
    "landing_url = client.v2.client_v2.url.split('/v2')[0] + '/projects/' + project_name\n",
    "print(f'You can now navigate to {landing_url} to compare drift calculated by weighted and non-weighted models.')\n",
    "print(f'You will notice that the class-weighted drift chart is able to surface the uptick in fraud.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddler examples have moved! [Deprecation Notice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dear user thank you for using fiddler product, we appreciate your time! We have moved the examples to a new github repo located at the following link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# [New fiddler-examples repo](https://github.com/fiddler-labs/fiddler-examples)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddler Quick Start Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide will walk you through the basic onboarding steps required to use Fiddler for production model monitoring and explainability. API documentation can be found [here](https://docs.fiddler.ai/api-reference/python-package/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Zero: Packages and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid import misses, we will have most package imports in this section. \n",
    "\n",
    "For the purposes of this tutorial, we will be training and utilizing a model that uses `scikit-learn` version `0.21.2`. We have provided a cell that can be used to verify the current version of `scikit-learn` to ensure a smooth tutorial experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn==0.21.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pickle\n",
    "import sklearn\n",
    "import shutil\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if `scikit-learn` version is `0.21.2`\n",
    "import sklearn\n",
    "\n",
    "if sklearn.__version__ != '0.21.2':\n",
    "    raise Exception('Please use sklearn version 0.21.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step One: Client Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will install the [Fiddler Python package](https://pypi.org/project/fiddler-client/) and establish an API connection to our Fiddler instance.\n",
    "\n",
    "This Python client is a powerful way to:\n",
    "- Upload the dataset and model to Fiddler\n",
    "- Ingest production events to Fiddler\n",
    "\n",
    "This can be done from a Jupyter Notebook or any python editor that you use to load data and build models.\n",
    "\n",
    "<img src=\"images/qs_d1.png\" width=700 height=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize the client object by specifying:\n",
    "- The `url`: url is the fiddler URL that you have been provided to access. Usually of the form ‘XXXXX.fiddler.ai’. Contact us if you don’t have it\n",
    "- The `org_id`: organization id is an identifier for the account. See Fiddler_URL/settings/general to find this id (listed as \"Organization ID\")\n",
    "<img src=\"images/org_id.png\" width=800 height=800 />\n",
    "- The `auth_token`: this token is used to authenticate access. See Fiddler_URL/settings/credentials to find, create, or change this token\n",
    "<img src=\"images/auth_token.png\" width=800 height=800 />\n",
    "\n",
    "You can also save this config as a file called `fiddler.ini` in the same folder as the notebook/script. That saves you from specifying the parameters in every notebook and script.\n",
    "<img src=\"images/fiddler_ini.png\" width=800 height=800 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiddler-client==0.6.8;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fiddler.ini\n",
    "\n",
    "[FIDDLER]\n",
    "url = http://host.docker.internal:4100\n",
    "org_id = onebox\n",
    "auth_token = YOUR_TOKEN_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "\n",
    "# client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=auth_token)\n",
    "client = fdl.FiddlerApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiddler has three primary constructs, namely projects, datasets and models. This diagram illustrates the relationship between the three.\n",
    "<img src=\"images/qs_d2.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fiddler client provides a number of methods. API documentation can be found [here](https://docs.fiddler.ai/api-reference/python-package/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Two: Create Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a project, a convenient container for housing the models and datasets associated with a given ML use case.\n",
    "\n",
    "For the purposes of a full quick start, it is best to create a `project_id` with a unique name to best track your progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'quickstart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our project using project_id\n",
    "if project_id not in client.list_projects():\n",
    "    client.create_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Three: Upload Baseline Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will upload the datasets that will serve as baselines for various product capabilities, including monitoring of model performance, prediction & feature drift, and data errors; generating prediction-level (point) and model-level (global) explanations; and calculating various bias metrics.\n",
    "\n",
    "We recommend using the model's training and test set for the most faithful and actionable metrics. In addition to the model's features and labels, Fiddler requires a few additional attributes to unlock its full suite of capabilities:\n",
    "\n",
    "*   Model predictions (Mandatory: serves as a baseline for prediction drift)\n",
    "*   Model decisions* (Optional: used to monitor model decsions over time, e.g. loan approved vs denied. The data uploaded initially can be random)\n",
    "*  Model metadata* (Any additional fields relevant for model analysis. In the event you intend to use Fiddler to detect model bias, include any relevant protected attributes here, e.g. gender, race, age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data you are going to use for training your model. For this tutorial, we will be using an auto insurance dataset that can be found [here](https://www.kaggle.com/somjee/auto-insurance-customerlifetimevalue?select=data.csv). \n",
    "\n",
    "**Note**: In the next cell, we will be making some modifications to the field names to better fit model training in later steps. We will also be adding a decisions column to our dataset. In total, we will be:\n",
    "- Renaming field `State` to `Location State` (State is a reserved word for models, so this change was required for model training)\n",
    "- Addition of a new `high_value` field to act as our decision column\n",
    "- Renaming columns to be a more library-friendly schema (lowercase and replacing spaces with underscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/somjee/auto-insurance-customerlifetimevalue?select=data.csv\n",
    "df = pd.read_csv('/app/fiddler_samples/samples/datasets/auto_insurance/data.csv')\n",
    "df = df.rename(columns={\"State\": \"Location State\"})\n",
    "df.columns = [x.lower().replace(' ', '_') for x in df.columns]\n",
    "\n",
    "# Adding a decision column to our data. In this case, we deem a 'high_value' customer as\n",
    "# one with customer_lifetime_value >= 5000\n",
    "df = df.assign(high_value=['Yes' if x >= 5000 else 'No' for x in df['customer_lifetime_value']])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into Train/Test\n",
    "\n",
    "Now we will split our dataset into a train/test set to be used in training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.8,random_state=200)\n",
    "df_test = df.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload a model, you first need to upload a sample of the data of the model’s inputs, targets, and additional metadata that might be useful for model analysis. This data sample helps us (among other things) to infer the model schema and the data types and values range of each feature.\n",
    "- This sample has to be a flat table that can be loaded as a pandas DF (```upload_dataset()```).\n",
    "- This input data sample is used for many downstream functions in Fiddler, including: shapley value methods, what-if (ICE) plots, PDP plots, drift, outliers, and data integrity.\n",
    "- We suggest uploading a sample of the model’s training data as it’s the most meaningful for the tasks listed above. For example, model outliers should be ideally based on the training data as that’s the data the model has seen. \n",
    "- You can upload multiple datasets with string identifiers, but we currently do not ascribe any meaning to those. For example: ```dataset={'data': df}``` or ```dataset={'train': train_df, 'test': test_df}```.\n",
    "- Currently we support two input types:\n",
    "    - Tabular\n",
    "    - Single string text, meaning text data in a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'auto_insurance'\n",
    "dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a schema for our dataset, and upload the dataset to Fiddler. \n",
    "\n",
    "If the `dataset_id` is already uploaded previously, we can fetch and use the schema from there. In the case that you wish to delete and reupload a dataset (e.g. different fields, different `max_inferred_cardinality`, etc.), we've included some commented code that shows how to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve dataset if already uploaded\n",
    "if dataset_id in client.list_datasets():\n",
    "    df_schema = client.get_dataset_info(dataset_id)\n",
    "else:\n",
    "    df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=1000)\n",
    "    upload_result = client.upload_dataset(\n",
    "        dataset={'train': df_train,\n",
    "                 'test': df_test},\n",
    "        dataset_id=dataset_id,\n",
    "        info=df_schema)\n",
    "\n",
    "\"\"\"\n",
    "# Delete dataset and reupload fresh\n",
    "if dataset_id in client.list_datasets():\n",
    "    client.delete_dataset(dataset_id)\n",
    "else:\n",
    "    df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=1000)\n",
    "    upload_result = client.upload_dataset(\n",
    "        dataset={'train': df_train,\n",
    "                 'test': df_test},\n",
    "        dataset_id=dataset_id,\n",
    "        info=df_schema)\n",
    "\"\"\"\n",
    "\n",
    "df_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Four: Create and Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, in the dataset upload step we did not ask for the model’s features and targets, or any model specific information. That’s because we allow for linking multiple models to a given dataset schema. Hence we require an Infer model schema step which helps us know the features relevant to the model and the model task. Here you can specify the input features, the target column, decision columns and metadata columns, and also the type of model.\n",
    "- We can infer the model task from the target column, or it can explicitly set. Currently we support three model types:\n",
    "    - Regression\n",
    "    - Binary Classification\n",
    "    - Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'customer_lifetime_value'\n",
    "continuous_features = ['income', 'monthly_premium_auto', 'months_since_last_claim', 'months_since_policy_inception',\n",
    "                        'number_of_open_complaints', 'number_of_policies', 'total_claim_amount']\n",
    "categorical_features = ['location_state', 'employmentstatus', 'policy_type', 'policy', 'vehicle_class','vehicle_size']\n",
    "\n",
    "feature_columns = list(continuous_features + categorical_features)\n",
    "metadata_cols = ['gender']\n",
    "decision_cols = ['high_value']\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=client.get_dataset_info(dataset_id),\n",
    "    target=target, \n",
    "    features=feature_columns,\n",
    "    metadata_cols=metadata_cols,\n",
    "    decision_cols=decision_cols,\n",
    "    display_name='Gradient Boosting Regressor',\n",
    "    description='this is a GradientBoostingRegressor model from the tutorial',\n",
    ")\n",
    "\n",
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train your model. For this model, we will be creating a Pipeline that will transform the data passed in, and then run that data through a gradient boosting regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references\n",
    "# https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "# https://songxia-sophia.medium.com/two-machine-learning-algorithms-to-predict-xgboost-neural-network-with-entity-embedding-caac68717dea\n",
    "# https://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import sklearn.pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "target = 'customer_lifetime_value'\n",
    "continuous_features = ['income', 'monthly_premium_auto', 'months_since_last_claim', 'months_since_policy_inception',\n",
    "                        'number_of_open_complaints', 'number_of_policies', 'total_claim_amount']\n",
    "categorical_features = ['location_state', 'employmentstatus', 'policy_type', 'policy', 'vehicle_class','vehicle_size']\n",
    "\n",
    "category_transformer = sklearn.pipeline.Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('cat', category_transformer, categorical_features),\n",
    "                                               ('cont', StandardScaler(), continuous_features)])\n",
    "\n",
    "model = GradientBoostingRegressor(learning_rate=0.1,\n",
    "                                  n_estimators=100,\n",
    "                                  max_depth=7)\n",
    "model_pipeline = sklearn.pipeline.Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                                  ('model', model)])\n",
    "\n",
    "model_pipeline.fit(df_train.loc[:, df_train.columns !=  target], df_train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Five: Create Model Directory and Uploading Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before uploading our model, we need to save the model and any pre-processing step you had on the input features (for example Categorical encoder, Tokenization, ...).  \n",
    "We currently support the following stored model formats:\n",
    "- For sklearn API based models, pickled models, or any storage format that you can load in the package.py (details below).\n",
    "- For TF, we support TF Saved Model and Keras .h5   \n",
    "\n",
    "Note:\n",
    "- Keras models have to have their input tensor differentiable if Integrated Gradients support is desired\n",
    "- We also need to save the data preprocessing pipeline code, if any. This will be accessed in the package.py\n",
    "\n",
    "In total, we will have a `model.yaml`, a `model.pkl`, and a `package.py` file within our model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model directory\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "model_id = 'gradient_boosting_regressor'\n",
    "model_dir = pathlib.Path(model_id)\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model and schema (`model.pkl` and `model.yaml`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write model schema file to model directory\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "# Saving model\n",
    "with open(model_dir / 'model.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(model_pipeline, pkl_file)\n",
    "\n",
    "# Saving schema\n",
    "with open(model_dir / 'model.yaml', 'w') as yaml_file:\n",
    "    yaml.dump({'model': model_info.to_dict()}, yaml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `package.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`package.py` is a Python module that\n",
    "\n",
    "- Facilitates model loading\n",
    "- Implements interfaces necessary for the Fiddler platform to interact with models.\n",
    "\n",
    "This provides the flexibility to enable Fiddler to support a wide variety of complex models. More information can be found [here](https://docs.fiddler.ai/api-reference/package-py/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gradient_boosting_regressor/package.py\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "PACKAGE_PATH = Path(__file__).parent\n",
    "TARGET = 'customer_lifetime_value'\n",
    "PREDICTION = 'predicted_customer_lifetime_value'\n",
    "\n",
    "class GBRegressor:\n",
    "    \"\"\"A Gradient Boosting Regressor predictor for auto_insurance data.\n",
    "       This loads the predictor once and runs for each call to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path, output_column=None):\n",
    "        \"\"\"\n",
    "        :param model_path: The directory where the model is saved.\n",
    "        :param output_column: list of column name(s) for the output.\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.output_column = output_column\n",
    "\n",
    "        file_path = os.path.join(self.model_path, 'model.pkl')\n",
    "        with open(file_path, 'rb') as file:\n",
    "            self.model = pkl.load(file)\n",
    "\n",
    "    def predict(self, input_df):\n",
    "        return pd.DataFrame(\n",
    "            self.model.predict(input_df.loc[:, input_df.columns != TARGET]), \n",
    "            columns=self.output_column\n",
    "        )\n",
    "\n",
    "def get_model():\n",
    "    return GBRegressor(model_path=PACKAGE_PATH, output_column=[PREDICTION])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model Package\n",
    " \n",
    "This step finds issues with the `package.py` composed above to enable easy debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiddler import PackageValidator\n",
    "\n",
    "validator = PackageValidator(model_info, df_schema, model_dir)\n",
    "passed, errors = validator.run_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the parts that we need, we can go ahead and upload the model to the Fiddler platform. You can use the [upload_model_package](https://docs.fiddler.ai/api-reference/python-package/#upload-model-package) to upload this entire directory in one shot. We need the following for uploading a model:\n",
    "- The `path` to the directory\n",
    "- The `project_id` to which the model belongs\n",
    "- The `model_id`, which is the name you want to give the model. You can access it in Fiddler henceforth via this ID\n",
    "- The `dataset` which the model is linked to (optional)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first delete the model if it already exists in the project\n",
    "if model_id in client.list_models(project_id):\n",
    "    client.delete_model(project_id, model_id)\n",
    "    print('Model deleted')\n",
    "    \n",
    "client.upload_model_package(artifact_path=model_dir, project_id=project_id, model_id=model_id)\n",
    "f\"Project '{project_id} now contains model '{model_id}'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's trigger the model to run its predictions by calling the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.trigger_model_predictions(project_id, model_id, dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test out our model by interfacing with the client and calling [run model](https://docs.fiddler.ai/api-reference/python-package/#run-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true = df_test['customer_lifetime_value']\n",
    "y_test_pred = client.run_model(project_id, model_id, df_test, log_events=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "test_mse = mean_squared_error(y_true, y_pred)\n",
    "test_r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f'MSE: {test_mse}\\nR2: {test_r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Six: Simulate Monitoring Traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will be simulating traffic to send for our model monitoring by using [publish_event](https://docs.fiddler.ai/api-reference/python-package/#publish-event). This will be the equivalent of running our model separately on some data, and either sending to Fiddler then, or saving this information to a log and sending at a later point.\n",
    "\n",
    "For this demonstration, we will be going with a log-related approach. This log contains rows corresponding to:\n",
    "\n",
    "- inputs \n",
    "- predictions\n",
    "- labels (targets)\n",
    "- decisions\n",
    "\n",
    "We can find the fields that will be utilized by consulting our `ModelInfo` object:\n",
    "\n",
    "```\n",
    "ModelInfo:\n",
    "      display_name: Gradient Boosting Regressor \\\n",
    "      description: this is a GradientBoostingRegressor model from the tutorial\n",
    "      input_type: ModelInputType.TABULAR\n",
    "      model_task: ModelTask.REGRESSION\n",
    "-->   inputs:\n",
    "                                   column     dtype count(possible_values)  \\\n",
    "        0                  location_state  CATEGORY                      5   \n",
    "        1                employmentstatus  CATEGORY                      5   \n",
    "        2                          income   INTEGER                          \n",
    "        3            monthly_premium_auto   INTEGER                          \n",
    "        4         months_since_last_claim   INTEGER                          \n",
    "        5   months_since_policy_inception   INTEGER                          \n",
    "        6       number_of_open_complaints   INTEGER                          \n",
    "        7              number_of_policies   INTEGER                          \n",
    "        8                     policy_type  CATEGORY                      3   \n",
    "        9                          policy  CATEGORY                      9   \n",
    "        10             total_claim_amount     FLOAT                          \n",
    "        11                  vehicle_class  CATEGORY                      6   \n",
    "        12                   vehicle_size  CATEGORY                      3                    \n",
    "-->   outputs\n",
    "                                      column  dtype count(possible_values)  \\\n",
    "        0  predicted_customer_lifetime_value  FLOAT                          \n",
    "\n",
    "          is_nullable value_range  \n",
    "        0       False       * - *  \n",
    "      metadata:\n",
    "           column     dtype  count(possible_values) is_nullable value_range\n",
    "        0  gender  CATEGORY                       2       False            \n",
    "-->   decisions:\n",
    "               column     dtype  count(possible_values) is_nullable value_range\n",
    "        0  high_value  CATEGORY                       2       False            \n",
    "-->   targets: [Column(name=\"customer_lifetime_value\", data_type=DataType.FLOAT, possible_values=None, \n",
    "                is_nullable=False, value_range_min=1898.007675, value_range_max=83325.38119)]\n",
    "                  misc:{}\n",
    "    \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log = pd.read_csv('/app/fiddler_samples/samples/datasets/auto_insurance/event_log.csv')\n",
    "event_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will be simulating traffic to send for our model monitoring by using \n",
    "[publish_event](https://docs.fiddler.ai/api-reference/python-package/#publish-event). \n",
    "This will be the equivalent of running our model separately on data, and either \n",
    "sending to Fiddler then, or saving this information to a log and sending at a later point.\n",
    "\n",
    "For this demonstration, we will be going with a log-related approach. \n",
    "This log contains rows that have inputs and predictions. \n",
    "To most accurately simulate this as a time-series event, we will generate a timestamp and send an event every 5 minutes. Real data will ideally have a timestamp related to when the event took place; otherwise, the current \n",
    "time will be used.\n",
    "\n",
    "We can send the inputs, outputs, targets as well as decisions variables.\n",
    "\n",
    "**Note**: The timestamp must be in UTC milliseconds. See \n",
    "[here](https://docs.fiddler.ai/api-reference/python-package/#publish-event) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "NUM_EVENTS_TO_SEND = 500\n",
    "\n",
    "FIVE_MINUTES_MS = 300000\n",
    "FIFTEEN_MINUTES_MS = FIVE_MINUTES_MS * 3\n",
    "ONE_DAY_MS = 8.64e+7\n",
    "start_date = round(time.time() * 1000) - (ONE_DAY_MS * 8)\n",
    "print(datetime.datetime.fromtimestamp(start_date/1000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this dataframe into a list of dictionary events, where each event is its own dictionary\n",
    "event_list_dict = event_log.sample(n=NUM_EVENTS_TO_SEND, random_state=42).to_dict(orient='records') \n",
    "\n",
    "for ind, event_dict in enumerate(event_list_dict):\n",
    "    event_time = start_date + ind * FIVE_MINUTES_MS\n",
    "    result = client.publish_event(project_id,\n",
    "                                  model_id,\n",
    "                                  event_dict,\n",
    "                                  event_time_stamp=event_time,\n",
    "                                  event_id=str(ind + 100),\n",
    "                                  update_event=False)\n",
    "    \n",
    "    readable_timestamp = datetime.datetime.fromtimestamp(event_time/1000.0)\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    print(f'Sending {ind+1} / {NUM_EVENTS_TO_SEND} \\n{readable_timestamp} UTC: \\n{event_dict}')\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In the case that labels are ingested in a future point, an event can be updated by calling:\n",
    "\n",
    "- `res = fiddler_api.publish_event(project_id, model_id, event, event_id=customer_id, update_event=True, event_time_stamp=row['__occurred_at'])`\n",
    "\n",
    "By setting the `update_event` flag to be true, the event identifed by `event_id` will be updated with whatever additional information you pass in through `event`, including a target label. See [here](https://docs.fiddler.ai/api-reference/python-package/#publish-event) for more details.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing Monitoring Traffic\n",
    "We can now consult our Fiddler instance to visualize our monitoring results. We can see our newly created project within the Projects Overview section:\n",
    "\n",
    "<img src=\"images/qs_projects.png\" width=1000 height=1000 />\n",
    "\n",
    "Within our project, we can click `gradient_boosting_regressor` to see our model we created. From there, we can see the traffic that reflects the events we sent by going to the Monitor Section at the top:\n",
    "\n",
    "<img src=\"images/qs_monitoring.png\" width=1000 height=1000 />\n",
    "\n",
    "For a walkthrough to learn more about navigating the product, please consult our [Product Tour](https://docs.fiddler.ai/product-tour/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

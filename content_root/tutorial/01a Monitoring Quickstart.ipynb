{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiddler Quick Start Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide will walk you through the basic onboarding steps required to use Fiddler for production model monitoring and explainability. API documentation can be found [here](https://api.fiddler.ai)\n",
    "\n",
    "Using Fiddler is a simple 3-step process visualized here -\n",
    "\n",
    "<img src=\"images/QS_flowchart.png\" width=500 height=300/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required packages at the beginning\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import shutil\n",
    "import yaml\n",
    "import datetime\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from random import sample, randint\n",
    "\n",
    "import fiddler as fdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Zero: Client Setup (Connecting to the URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize the client object by specifying:\n",
    "- The `url`: url is the fiddler URL that you have been provided to access. Usually of the form ‘XXXXX.fiddler.ai’. Contact us if you don’t have it\n",
    "- The `org_id`: organization id is an identifier for the account. See Fiddler_URL/settings/general to find this id (listed as \"Organization ID\")\n",
    "<img src=\"images/org_id.png\" width=800 height=600 />\n",
    "- The `auth_token`: this token is used to authenticate access. See Fiddler_URL/settings/credentials to find, create, or change this token\n",
    "<img src=\"images/auth_token.png\" width=800 height=600 />\n",
    "\n",
    "You can also save this config as a file called `fiddler.ini` in the same folder as the notebook/script. That saves you from specifying the parameters in every notebook and script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fiddler.ini\n",
    "\n",
    "[FIDDLER]\n",
    "url = https://trial.fiddler.ai\n",
    "org_id = company_name\n",
    "auth_token = 43_character_length_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = fdl.FiddlerApi(url='https://trial.fiddler.ai', org_id='your_org_id', auth_token=\"your_auth_token\")\n",
    "client = fdl.FiddlerApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step One: Upload Baseline Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a project, a convenient container for housing the models and datasets associated with a given ML use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'quickstart' # project_id may only contain lowercase letters, numbers or underscore\n",
    "client.create_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will upload the dataset (training data or a representative sample of the same) that will serve as baselines for various product capabilities, including monitoring and explainability of the models. For this tutorial, we will be using a cleaned version of auto insurance dataset that can be found [here](https://www.kaggle.com/somjee/auto-insurance-customerlifetimevalue?select=data.csv). We are predicting whether a customer would be high value or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../samples/datasets/auto_insurance/data_cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the dataset\n",
    "dataset_id = 'auto_insurance' # dataset_id may only contain lowercase letters, numbers or underscore\n",
    "client.upload_dataset(project_id=project_id,dataset_id = dataset_id, dataset = {'baseline' : df}, \n",
    "                      info = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=1000) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Two: Add Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, in the dataset upload step we did not ask for the model’s features and targets, or any model specific information. That’s because we allow for linking multiple models to a given dataset schema. Hence we require an Infer model schema step which helps us know the features relevant to the model and the model task. Here you can specify the input features, the target column, decision columns and metadata columns, and also the type of model.\n",
    "- We can infer the model task from the target column, or it can explicitly set. Currently we support three model types:\n",
    "    - Regression\n",
    "    - Binary Classification\n",
    "    - Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'high_value_classifier' # model_id may only contain lowercase letters, numbers or underscore\n",
    "\n",
    "outputs = ['probability_high_value'] # output of the model\n",
    "target = 'high_value' # we're predicting whether the customer is high value (1) or not (0)\n",
    "decision_cols = ['Campaign_A'] # Based on the predicted high_value - should we send the customer this campaign\n",
    "input_features = df.drop(['probability_high_value', 'high_value','Campaign_A'], axis = 1).columns\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=client.get_dataset_info(project_id, dataset_id),\n",
    "    features = input_features,\n",
    "    target=target, \n",
    "    decision_cols=decision_cols,\n",
    "    outputs=outputs,\n",
    "    input_type=fdl.ModelInputType.TABULAR,\n",
    "    model_task=fdl.ModelTask.BINARY_CLASSIFICATION,\n",
    "    display_name='High value prediction model',\n",
    "    description='This is a Binary Classification model from the tutorial',\n",
    ")\n",
    "\n",
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_model(\n",
    "    project_id=project_id,\n",
    "    model_id=model_id, \n",
    "    dataset_id=dataset_id,\n",
    "    model_info=model_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Three: Simulate Monitoring Traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming data example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will be simulating traffic and monitoring it using [publish_event](https://api.fiddler.ai/#publish-event). This will be the equivalent of running our model separately on some data, and either sending to Fiddler then (streaming approach), or saving this information to a log and sending at a later point (using a batch upload of the logs).\n",
    "\n",
    "For this demonstration, we will be going with a streaming approach. We will utilize a log containing rows with fields corresponding to:\n",
    "\n",
    "- inputs / features\n",
    "- outputs / predictions\n",
    "- targets / ground truth / labels\n",
    "- decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_log = pd.read_csv('../samples/datasets/auto_insurance/event_log.csv')\n",
    "event_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will publish these rows as events. To most accurately simulate this as a time-series event, we will be sending in each row by adding a generated timestamp with it. Real data will ideally have a timestamp related to when the event took place; otherwise, the current time will be used.\n",
    "\n",
    "**Note**: The timestamp must be in UTC milliseconds. See [here](https://api.fiddler.ai/#publish-event) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAYS = 10\n",
    "INTERVAL_MINS = 10\n",
    "INTERVAL_MS = INTERVAL_MINS*60*1e3\n",
    "NUM_EVENTS_TO_SEND = int(24*60/INTERVAL_MINS)*DAYS # publish an event every 10minutes for 10 days\n",
    "ONE_DAY_MS = 8.64e+7\n",
    "start_date = round(time.time() * 1000) - (ONE_DAY_MS * DAYS)\n",
    "        \n",
    "# Convert this dataframe into a list of dictionary events, where each event is its own dictionary\n",
    "event_list_dict = event_log.sample(n=NUM_EVENTS_TO_SEND).to_dict(orient='records') \n",
    "\n",
    "for ind, event_dict in enumerate(event_list_dict):\n",
    "    event_ms_time_stamp = start_date + ind * INTERVAL_MS\n",
    "    client.publish_event(project_id, model_id, event_dict, event_timestamp=event_ms_time_stamp)\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "    readable_timestamp = datetime.datetime.fromtimestamp(event_ms_time_stamp/1000.0)\n",
    "    \n",
    "    print(f'Sending {ind+1} / {NUM_EVENTS_TO_SEND} \\n{readable_timestamp} UTC: \\n{event_dict}')\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In the case that labels are ingested in a future point, an event can be updated by calling publish_event with update_event = True. See [here](https://api.fiddler.ai/#publish-event) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Monitoring Traffic\n",
    "We can now consult our Fiddler instance to visualize our monitoring results. We can see our newly created project within the Projects Overview section:\n",
    "\n",
    "<img src=\"images/qs_projects_list.png\" width=1000 height=1000 />\n",
    "\n",
    "\n",
    "\n",
    "Within the project, you can click on `high_value_classifier` to view the details of the model that was registered. From there, we can see the traffic that reflects the events we sent by going to the Monitor Section at the top:\n",
    "\n",
    "<img src=\"images/qs_monitoring_look.png\" width=1000 height=1000 />\n",
    "\n",
    "For a walkthrough to learn more about navigating the product, please consult our [Product Tour](https://docs.fiddler.ai/product-tour/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

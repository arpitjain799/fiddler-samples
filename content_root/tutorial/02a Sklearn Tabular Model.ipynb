{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload a Sklearn Regression Model\n",
    "This tutorial example shows how to train a wine quality model using sklearn and upload dataset and model onto Fiddler Platform using Fiddler Client API and how to serve predictions from that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Fiddler Client\n",
    "We begin this section as usual by establishing a connection to our\n",
    "Fiddler instance. We can establish this connection either by specifying \n",
    "our credentials directly, or by utilizing our `fiddler.ini` file. More\n",
    "information can be found in the [setup](https://github.com/fiddler-labs/fiddler-samples/blob/master/content_root/tutorial/00%20Install%20%26%20Setup.ipynb) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "\n",
    "# client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=auth_token)\n",
    "client = fdl.FiddlerApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "Here we will load in our baseline dataset from a csv called `train.csv`. We will\n",
    "also create a schema using this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../samples/datasets/winequality/train.csv')\n",
    "df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a project, a convenient container for housing the models and datasets associated with a given ML use case.\n",
    "Uploading our dataset in the next step will depend on our created project's `project_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'tutorial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our project using project_id\n",
    "if project_id not in client.list_projects():\n",
    "    client.create_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Dataset\n",
    "To upload a model, you first need to upload a sample of the data of the model’s \n",
    "inputs, targets, and additional metadata that might be useful for model analysis. \n",
    "This data sample helps us (among other things) to infer the model schema and the \n",
    "data types and values range of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'wine_quality' not in client.list_datasets(project_id):\n",
    "    upload_result = client.upload_dataset(\n",
    "        project_id=project_id,\n",
    "        dataset={'train': df}, \n",
    "        dataset_id='wine_quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Schema\n",
    "As you must have noted, in the dataset upload step we did not ask for the model’s \n",
    "features and targets, or any model specific information. That’s because we \n",
    "allow for linking multiple models to a given dataset schema. Hence we require \n",
    "an Infer model schema step which helps us know the features relevant to the \n",
    "model and the model task. Here you can specify the input features, the target \n",
    "column, decision columns and metadata columns, and also the type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'quality'\n",
    "train_input = df.drop(columns=['row_id', 'quality'])\n",
    "train_target = df[target]\n",
    "\n",
    "feature_columns = list(train_input.columns)\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=client.get_dataset_info(project_id, 'wine_quality'),\n",
    "    target=target, \n",
    "    features=feature_columns,\n",
    "    display_name='sklearn model',\n",
    "    description='this is a sklearn model from tutorial'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Fiddler currently supports scikit-learn version 0.21.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "assert sklearn.__version__ == '0.21.2', 'Please change sklearn version to 0.21.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "regressor = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "full_model = sklearn.pipeline.Pipeline(steps=[\n",
    "        ('standard_scaling', sklearn.preprocessing.StandardScaler()),\n",
    "        ('model_name', regressor),\n",
    "    ])\n",
    "\n",
    "full_model.fit(train_input, train_target)\n",
    "full_model.predict(train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Schema\n",
    "Next step, we need to save the model and any pre-processing step you had \n",
    "on the input features (for example Categorical encoder, Tokenization, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "project_id = 'tutorial'\n",
    "model_id = 'wine_quality_model'\n",
    "\n",
    "# create temp dir\n",
    "model_dir = pathlib.Path(model_id)\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir.mkdir()\n",
    "\n",
    "# save model\n",
    "with open(model_dir / 'model.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(full_model, pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write package.py wrapper\n",
    "A wrapper is needed between Fiddler and the model. This wrapper can be used to \n",
    "translate the inputs and outputs to fit what the model expects and what Fiddler \n",
    "is able to consume. More information can be found [here](https://api.fiddler.ai/#package-py/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile wine_quality_model/package.py\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "PACKAGE_PATH = Path(__file__).parent\n",
    "\n",
    "class SklearnModelPackage:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.is_classifier = False\n",
    "        self.is_multiclass = False\n",
    "        self.output_columns = ['predicted_quality']\n",
    "        with open(PACKAGE_PATH / 'model.pkl', 'rb') as infile:\n",
    "            self.model = pickle.load(infile)\n",
    "    \n",
    "    def predict(self, input_df):\n",
    "        if self.is_classifier:\n",
    "            if self.is_multiclass:\n",
    "                predict_fn = self.model.predict_proba\n",
    "            else:\n",
    "                def predict_fn(x):\n",
    "                    return self.model.predict_proba(x)[:, 1]\n",
    "        else:\n",
    "            predict_fn = self.model.predict\n",
    "        return pd.DataFrame(predict_fn(input_df), columns=self.output_columns)\n",
    "    \n",
    "def get_model():\n",
    "    return SklearnModelPackage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate model package\n",
    "\n",
    "This verifies consistency between `df_schema`, `model_info`, and `package.py`; and performs local functional tests on the wrapped model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiddler import PackageValidator\n",
    "validator = PackageValidator(model_info, df_schema, model_dir)\n",
    "passed, errors = validator.run_chain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model\n",
    "Now that we have all the parts that we need, we can go ahead and upload the model to the Fiddler platform. You can use the `add_model_artifact` to upload this entire directory in one shot. We need the following for uploading a model:\n",
    "- The `path` to the directory\n",
    "- The `project_id` to which the model belongs\n",
    "- The `model_id`, which is the name you want to give the model. You can access it in Fiddler henceforth via this ID\n",
    "- The `dataset_id` which the model is linked to  \n",
    "\n",
    "In total, we will have a `*.pkl`, and a `package.py` file within our model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if project_id not in client.list_projects():\n",
    "    client.create_project(project_id)\n",
    "client.delete_model(project_id, model_id)\n",
    "client.add_model(project_id=project_id, model_id=model_id, dataset_id=dataset_id, model_info=model_info)\n",
    "client.add_model_artifact(model_dir=model_dir, project_id=project_id, model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model\n",
    "Now, let's test out our model by interfacing with the client and \n",
    "calling [run model](https://api.fiddler.ai/#run-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_input = train_input[0: 10]\n",
    "result = client.run_model(project_id, model_id, prediction_input, log_events=True)\n",
    "result\n",
    "dir(prediction_input.dtypes)\n",
    "\n",
    "prediction_input.dtypes.keys\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

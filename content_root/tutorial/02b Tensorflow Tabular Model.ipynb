{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Fiddler Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python client is a powerful way to:\n",
    "- Upload the dataset and model to Fiddler\n",
    "- Ingest production events to Fiddler\n",
    "\n",
    "This can be done from a Jupyter Notebook or any python editor that you use to load data and build models.\n",
    "\n",
    "<img src=\"images/fiddler_client.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize the client object by specifying:\n",
    "- The url: url is the fiddler URL that you have been provided to access. Usually of the form ‘XXXXX.fiddler.ai’. Contact Fiddler if you don’t have it\n",
    "- The org_id: organization id is an identifier for the account. See Fiddler_URL/settings/general to find this id (listed as \"Organization ID\")\n",
    "<img src=\"images/org_id.png\" width=800 height=800 />\n",
    "- The auth_token: this token is used to authenticate access. See Fiddler_URL/settings/credentials to find, create, or change this token\n",
    "<img src=\"images/auth_token.png\" width=800 height=800 />\n",
    "\n",
    "You can also save this config as a file called fiddler.ini in the same folder as the notebook/script. That saves you from specifying the parameters in every notebook and script.\n",
    "<img src=\"images/fiddler_ini.png\" width=800 height=800 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiddler as fdl\n",
    "\n",
    "url = 'http://xxx.fiddler.ai'\n",
    "token = 'my_token'\n",
    "org_id = 'my_org_id'\n",
    "\n",
    "client = fdl.FiddlerApi(url=url, org_id=org_id, auth_token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiddler has three primary constructs, namely projects, datasets and models. This diagram illustrates the relationship between the three.\n",
    "<img src=\"images/projects_data_models.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fiddler client provides a number of methods.\n",
    "- List datasets: ```client.list_datasets()``` List the ids of all datasets in the org.\n",
    "- List projects: ```client.list_projects()``` List the ids of all projects in the org.\n",
    "- List models: ```client.list_models()``` List the names of all models in a project.\n",
    "- Create project: ```client.create_project()``` Create a new project.\n",
    "- Create model: ```client.create_model()``` Trigger auto-modeling on a dataset already uploaded to Fiddler.\n",
    "- Get dataset info: ```client.get_dataset_info()``` Get DatasetInfo for a dataset.\n",
    "- Get model info: ```client.get_model_info()``` Get ModelInfo for a model in a certain project.\n",
    "- Get dataset: ```client.get_dataset()``` Fetches data from a dataset on Fiddler.\n",
    "- Get slice: ```client.get_slice()``` Fetches data from Fiddler via a slice query (SQL query).\n",
    "- Delete dataset: ```client.delete_dataset()``` Permanently delete a dataset.\n",
    "- Delete model: ```client.delete_model()``` Permanently delete a model.\n",
    "- Delete model artifacts: ```client.delete_model_artifacts()``` Permanently delete a model artifacts.\n",
    "- Delete project: ```client.delete_project()``` Permanently delete a project.\n",
    "- Upload dataset: ```client.upload_dataset()``` Uploads a dataset to the Fiddler engine.\n",
    "- Upload dataset from a directory: ```client.upload_dataset_from_dir()``` Uploads a dataset from a directory to the Fiddler engine.\n",
    "- Run model: ```client.run_model()``` Executes a model in the Fiddler engine on a DataFrame.\n",
    "- Run explanation: ```client.run_explanation()``` Explains a model's prediction on a single instance.\n",
    "- Run feature importance: ```client.run_feature_importance()``` Get global feature importance for a model over a dataset.\n",
    "- Upload model sklearn: ```client.upload_model_sklearn()``` Uploads a subclass of sklearn.base.BaseEstimator to the Fiddler engine.\n",
    "- Upload model package: ```client.upload_model_package()``` Uploads a custom model object to the Fiddler engine along with custom glue-code for running the model.\n",
    "- Publish event: ```client.publish_event()``` Publishes an event to Fiddler Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'tf_tabular'\n",
    "dataset_id = 'heart_disease'\n",
    "model_id = 'heart_disease_tf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a project, a convenient container for housing the models and datasets associated with a given ML use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our project using project_id\n",
    "if project_id not in client.list_projects():\n",
    "    client.create_project(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data you are going to use for training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/app/fiddler_samples/samples/datasets/heart_disease/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload a model, you first need to upload a sample of the data of the model’s inputs, targets, and additional metadata that might be useful for model analysis. This data sample helps us (among other things) to infer the model schema and the data types and values range of each feature.\n",
    "- This sample has to be a flat table that can be loaded as a pandas DF (```upload_dataset()```) or saved as a csv (```upload_dataset_from_dir()```).\n",
    "- In this example age, sex, trestbps, chol, fbs, thalach, exang, oldpeak, slope are input features, and target is the target column for the model.\n",
    "- This input data sample is used for many downstream functions in Fiddler\n",
    "    - Shapley value methods - background data to simulate the missing of features\n",
    "    - What-if (ICE) plots - background data\n",
    "    - PDP plots - background data\n",
    "    - Drift - to serve as a baseline\n",
    "    - Outliers - to serve as a baseline\n",
    "    - Data integrity - to serve as a baseline\n",
    "- We suggest uploading a sample of the model’s training data as it’s the most meaningful for the tasks listed above. For example, model outliers should be ideally based on the training data as that’s the data the model has seen. \n",
    "- You can upload multiple datasets with string identifiers, but we currently do not ascribe any meaning to those. For example: ```dataset={'data': df}``` or ```dataset={'train': train_df, 'test': test_df}```.\n",
    "- Currently we support two input types:\n",
    "    - Tabular\n",
    "    - Single string text, meaning text data in a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema = fdl.DatasetInfo.from_dataframe(df, max_inferred_cardinality=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_id  not in client.list_datasets(project_id):\n",
    "    upload_result = client.upload_dataset(\n",
    "        project_id=project_id,\n",
    "        dataset={'data': df}, \n",
    "        dataset_id=dataset_id,\n",
    "        info=df_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you must have noted, in the dataset upload step we did not ask for the model’s features and targets, or any model specific information. That’s because we allow for linking multiple models to a given dataset schema. Hence we require an Infer model schema step which helps us know the features relevant to the model and the model task. Here you can specify the input features, the target column, decision columns and metadata columns, and also the type of model.\n",
    "- Currently we support only one target column. This is not to be confused with output columns, which can be more than one. \n",
    "- Decision columns specify the decisions made on the basis of the model’s predictions. For example, in a credit lending scenario, the business decision to give or not to give a loan based on the model’s output. This is helpful while monitoring models after deployment, to keep track of the business impact of the model.\n",
    "- Metadata is data that is not used by the model, but can be relevant for understanding the model’s behavior on different segments of the data. For example, gender, race, age and other such sensitive features may not be used in the model, but we can analyze along these dimensions post facto to understand if the model is biased.\n",
    "- We can infer the model task from the target column, or it can explicitly set. Currently we support three model types:\n",
    "    - Regression\n",
    "    - Binary Classification\n",
    "    - Multi-class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "feature_columns = list(df.drop(columns=['target']).columns)\n",
    "\n",
    "model_info = fdl.ModelInfo.from_dataset_info(\n",
    "    dataset_info=client.get_dataset_info(project_id, 'heart_disease'),\n",
    "    target=target, \n",
    "    features=feature_columns,\n",
    "    display_name='Keras Tabular IG',\n",
    "    description='this is a keras model using tabular data and IG enabled from tutorial',\n",
    "    model_task=fdl.ModelTask.BINARY_CLASSIFICATION,\n",
    "    preferred_explanation_method=fdl.ExplanationMethod.IG_FLEX\n",
    ")\n",
    "model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TensorFlow if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we support Sklearn version 0.21.2 and TF version 1.14  \n",
    "If you have another version, please contact Fiddler for assistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "assert tf.__version__=='2.5.0', 'Please change tensorflow version to 2.5.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = df.drop(columns=['target'])\n",
    "train_target = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(train_input.shape[1], ))\n",
    "activations = tf.keras.layers.Dense(32, activation='linear', use_bias=True)(inputs)\n",
    "activations = tf.keras.layers.Dense(128, activation=tf.nn.relu, use_bias=True)(activations)\n",
    "activations = tf.keras.layers.Dense(128, activation=tf.nn.relu, use_bias=True)(activations)\n",
    "activations = tf.keras.layers.Dense(1, activation='sigmoid', use_bias=True)(activations)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=activations, name='keras_model')\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(train_input, train_target.values, batch_size=32, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_input, train_target) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model and schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step, we need to save the model and any pre-processing step you had on the input features (for example Categorical encoder, Tokenization, ...).  \n",
    "We currently support the following stored model formats:\n",
    "- For sklearn API based models, pickled models, or any storage format that you can load in the package.py (details below).\n",
    "- For TF, we support TF Saved Model and Keras .h5   \n",
    "\n",
    "Note:\n",
    "- Keras models have to have their input tensor differentiable if Integrated Gradients support is desired\n",
    "- We also need to save the data preprocessing pipeline code, if any. This will be accessed in the package.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "# For demo purpose, let's save this model in Keras .h5 demo.\n",
    "\n",
    "# create temp dir\n",
    "model_dir = pathlib.Path(model_id)\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "model_dir_keras.mkdir()\n",
    "\n",
    "# save model\n",
    "model.save(str(model_dir / 'model.h5'), include_optimizer=False)\n",
    "\n",
    "# save model schema\n",
    "with open(model_dir / 'model.yaml', 'w') as yaml_file:\n",
    "    yaml.dump({'model': model_info.to_dict()}, yaml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write package.py and related wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import related wrappers\n",
    "We need to import the GEM wrapper for displaying the attributions. This file is stored in the utils directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('utils/GEM.py', model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile heart_disease_tf/package.py\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "from .GEM import GEMContainer, GEMSimple, GEMText\n",
    "\n",
    "\n",
    "class MyModel:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model_dir = pathlib.Path(__file__).parent\n",
    "\n",
    "        self.model = load_model(str(self.model_dir / 'model.h5'))\n",
    "        self.output_columns = ['predicted_target']\n",
    "        self.inputs = self.model.input.name\n",
    "    \n",
    "    def get_settings(self):\n",
    "        return {'ig_start_steps': 32,  # 32\n",
    "                'ig_max_steps': 4096,  # 2048\n",
    "                'ig_min_error_pct':5.0 # 1.0\n",
    "               }\n",
    "\n",
    "    def _transform_input(self, input_df):\n",
    "        return input_df\n",
    "\n",
    "    def get_ig_baseline(self, input_df):\n",
    "        \"\"\" This method is used to generate the baseline against which to compare the input. \n",
    "            It accepts a pandas DataFrame object containing rows of raw feature vectors that \n",
    "            need to be explained (in case e.g. the baseline must be sized according to the explain point).\n",
    "            Must return a pandas DataFrame that can be consumed by the predict method described earlier.\n",
    "        \"\"\"\n",
    "        return input_df*0\n",
    "\n",
    "    def predict(self, input_df):\n",
    "        transformed_input_df = self._transform_input(input_df)\n",
    "        pred = self.model.predict(transformed_input_df)\n",
    "        return pd.DataFrame(pred, columns=self.output_columns)\n",
    "    \n",
    "    def transform_to_attributable_input(self, input_df):\n",
    "        \"\"\" This method is called by the platform and is responsible for transforming the input dataframe\n",
    "            to the upstream-most representation of model inputs that belongs to a continuous vector-space.\n",
    "            For this example, the model inputs themselves meet this requirement.  For models with embedding\n",
    "            layers (esp. NLP models) the first attributable layer is downstream of that.\n",
    "        \"\"\"\n",
    "        transformed_input = self._transform_input(input_df)\n",
    "\n",
    "        return {self.inputs: input_df.values}\n",
    "    \n",
    "    def compute_gradients(self, attributable_input):\n",
    "        \"\"\" This method computes gradients of the model output wrt to the differentiable input. \n",
    "            If there are embeddings, the attributable_input should be the output of the embedding \n",
    "            layer. In the backend, this method receives the output of the transform_to_attributable_input() \n",
    "            method. This must return an array of dictionaries, where each entry of the array is the attribution \n",
    "            for an output. As in the example provided, in case of single output models, this is an array with \n",
    "            single entry. For the dictionary, the key is the name of the input layer and the values are the \n",
    "            attributions.\n",
    "        \"\"\"\n",
    "        gradients_by_output = []\n",
    "        attributable_input_tensor = {k: tf.identity(v) for k, v in attributable_input.items()}\n",
    "        gradients_dic_tf = self._gradients_input(attributable_input_tensor)\n",
    "        gradients_dic_numpy = dict([key, np.asarray(value)] for key, value in gradients_dic_tf.items()) \n",
    "        gradients_by_output.append(gradients_dic_numpy)\n",
    "        return gradients_by_output    \n",
    "    \n",
    "    def _gradients_input(self, x):\n",
    "        \"\"\"\n",
    "        Function to Compute gradients.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x)\n",
    "            preds = self.model(x)\n",
    "\n",
    "        grads = tape.gradient(preds, x)\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "    def project_attributions(self, input_df, attributions):\n",
    "        att = []\n",
    "        for ind, col in enumerate(input_df.columns):\n",
    "            val = input_df[col].values[0]\n",
    "            att.append(GEMSimple(display_name=col, feature_name=col,\n",
    "                                 value=float(val),\n",
    "                                 attribution=attributions[0][self.inputs][ind]))\n",
    "\n",
    "        gem_container = GEMContainer(contents=att)\n",
    "\n",
    "        explanations_by_output = {self.output_columns[0]: gem_container.render()}\n",
    "\n",
    "        return explanations_by_output\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = MyModel()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the parts that we need, we can go ahead and upload the model to the Fiddler platform. You can use the [upload_model_package](https://api.fiddler.ai/#upload-model-package) to upload this entire directory in one shot. We need the following for uploading a model:\n",
    "- The `path` to the directory\n",
    "- The `project_id` to which the model belongs\n",
    "- The `model_id`, which is the name you want to give the model. You can access it in Fiddler henceforth via this ID\n",
    "- The `dataset` which the model is linked to (optional)  \n",
    "\n",
    "In total, we will have a `model.yaml`, a model file, the GEM.py wrapper and a `package.py` file within our model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first delete the model if it already exists in the project\n",
    "if model_id in client.list_models(project_id):\n",
    "    client.delete_model(project_id, model_id)\n",
    "    print('Model deleted')\n",
    "    \n",
    "client.upload_model_package(model_dir, project_id, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_input = train_input[:10]\n",
    "client.run_model(project_id, model_id, prediction_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_point = df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.run_explanation(\n",
    "    project_id=project_id,\n",
    "    model_id=model_id, \n",
    "    df=selected_point, \n",
    "    dataset_id=dataset_id,\n",
    "    explanations='ig_flex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
